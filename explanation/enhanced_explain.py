# explanation/enhanced_explain.py
import os  
from langchain.chains import SequentialChain
from langchain.prompts import PromptTemplate
from langchain_community.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from dotenv import load_dotenv

load_dotenv()

# Ensure the OpenAI API key is set as an environment variable.
openai_api_key = os.getenv("OPENAI_API_KEY")
if not openai_api_key:
    raise ValueError("Please set your OPENAI_API_KEY environment variable.")

# Initialize the OpenAI LLM (non-streaming instance for the synchronous chain)
llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7, openai_api_key=openai_api_key)

# Define prompt for summarizing the review
summary_prompt = PromptTemplate(
    template="""
You are an expert reviewer. Summarize the key points of the following product review:

Review: {review}

Summary:
""",
    input_variables=["review"]
)

# Define prompt for generating a detailed explanation based on the summary and label
explanation_prompt = PromptTemplate(
    template="""
Based on the summary: {summary}
and the original review: {review},
provide a detailed explanation on why this review might be considered {label}.

Explanation:
""",
    input_variables=["summary", "review", "label"]
)

# Create LLM chains for each step
summary_chain = LLMChain(llm=llm, prompt=summary_prompt, output_key="summary")
explanation_chain = LLMChain(llm=llm, prompt=explanation_prompt, output_key="explanation")

# Combine the chains into a sequential chain
sequential_chain = SequentialChain(
    chains=[summary_chain, explanation_chain],
    input_variables=["review", "label"],
    output_variables=["summary", "explanation"]
)

def generate_enhanced_explanation(review: str, label: str) -> dict:
    """
    Generate both a summary and a detailed explanation for the given review and label using a sequential chain.
    
    :param review: The product review text.
    :param label: The predicted label ("Fake" or "Genuine").
    :return: A dict containing keys 'summary' and 'explanation'.
    """
    chain_input = {"review": review, "label": label}
    result = sequential_chain(chain_input)
    return result

def generate_explanation_stream(review: str, label: str):
    """
    Generate a summary synchronously and stream the detailed explanation tokens.
    
    :param review: The product review text.
    :param label: The predicted label ("Fake" or "Genuine").
    :return: A tuple where the first element is the summary text and the second is a token generator.
    """
    # Get summary (non-streaming)
    summary_result = summary_chain({"review": review, "label": label})
    summary = summary_result.get("summary", "")

    from queue import Queue
    import threading
    from langchain.callbacks.base import BaseCallbackHandler

    q = Queue()

    # Implement the required abstract methods to avoid instantiation error
    class StreamingCallbackHandler(BaseCallbackHandler):
        """
        Callback handler that streams tokens to a queue for asynchronous consumption.
        """
        def on_llm_new_token(self, token: str, **_kwargs) -> None:
            """
            Invoked for each new token generated by the language model.
            
            :param token: The newly generated token.
            """
            q.put(token)
        def on_llm_start(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_llm_end(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_llm_error(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_chain_start(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_chain_end(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_chain_error(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_tool_start(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_tool_end(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_tool_error(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_text(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_agent_action(self, *_args, **_kwargs):
            del _args, _kwargs
            pass
        def on_agent_finish(self, *_args, **_kwargs):
            del _args, _kwargs
            pass

    from langchain.callbacks.manager import CallbackManager
    streaming_llm = ChatOpenAI(
        model_name="gpt-4o",
        temperature=0.7,
        streaming=True,
        callback_manager=CallbackManager([StreamingCallbackHandler()]),
        openai_api_key=openai_api_key
    )
    streaming_explanation_chain = LLMChain(llm=streaming_llm, prompt=explanation_prompt, output_key="explanation")

    def run_explanation():
        """
        Execute the streaming explanation chain in a separate thread and put a sentinel into the queue when done.
        """
        streaming_explanation_chain({"summary": summary, "review": review, "label": label})
        q.put(None)  # Sentinel value

    thread = threading.Thread(target=run_explanation)
    thread.start()

    def token_generator():
        """
        Yield tokens from the queue until the None sentinel is encountered.
        """
        while True:
            token = q.get()
            if token is None:
                break
            yield token

    return summary, token_generator()

if __name__ == "__main__":
    sample_review = "I can't believe how terrible this product is! It stopped working within days."
    sample_label = "Fake"  # or "Genuine"
    summary, explanation_generator = generate_explanation_stream(sample_review, sample_label)
    print("Summary:", summary)
    print("Explanation:")
    for token in explanation_generator:
        print(token, end="", flush=True)
